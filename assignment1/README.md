# Assignment 1

Implement and test various SiLU implementations.

### PyTorch

```bash
$ uv run python assignment1/silu/silu_torch.py
================================================================================
PyTorch SiLU implementation and performance profiling
================================================================================
[+] Using device: cuda
[+] Tensor shape: torch.Size([8192, 8192])
[+] Running 10 warmup iterations...
[+] Performance averaging over 100 timed iterations...
[+] Exporting trace to torch_silu.json

================================================================================
Performance Numbers
================================================================================
[+] Time for SiLU: 0.004272367553710937 seconds
[+] Bandwidth: 125.66121834102947 GBps

================================================================================
Correctness Test
================================================================================
[+] Max difference: 0.00e+00
[+] Test closeness: PASS
$
```

### Triton

```bash
$ uv run python assignment1/silu/silu_triton_test.py
Using device: cuda

================================================================================
Correctness Test
================================================================================
Tensor Shape         Test Status          Max Diff
--------------------------------------------------------------------------------
(1024, 1024)         PASS ✅               4.77e-07
(2048, 2048)         PASS ✅               7.15e-07
(4096, 4096)         PASS ✅               9.54e-07
(8192, 8192)         PASS ✅               9.54e-07

================================================================================
Runtime Benchmark
================================================================================
Size                 PyTorch (ms)    Triton (ms)     Speedup
--------------------------------------------------------------------------------
(1024, 1024)         0.0576          0.0463          1.24
(2048, 2048)         0.2772          0.0904          3.07
(4096, 4096)         1.0697          0.2727          3.92
(8192, 8192)         4.3159          0.9986          4.32

================================================================================
Bandwidth Benchmark
================================================================================
Size                 PyTorch (GB/s)  Triton (GB/s)   Speedup
--------------------------------------------------------------------------------
(1024, 1024)         145.66          181.39          1.25
(2048, 2048)         121.04          371.19          3.07
(4096, 4096)         125.48          492.50          3.93
(8192, 8192)         124.39          537.63          4.32

$
```

### CUDA

```bash
TODO
```
